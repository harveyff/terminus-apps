metadata:
  title: Ollama
  description: Get up and running with large language models.

spec:
  fullDescription: |
    ## IMPORTANT NOTE ##
    This is a shared app. Once installed by the Olares Admin, all users in the cluster can use it through reference app.

    ## OVERVIEW ##
    Ollama is a user-friendly interface for running large language models (LLMs) locally. It is a valuable tool for researchers, developers, and anyone who wants to experiment with language models. With Ollama, you can easily download and run LLMs, customize and create your own, and chat with your LLMs using files on your device.

    Ollama supports a wide range of models, including:
    - Llama 3 8B
    - Llama 3 70B
    - Phi 3 Mini 3.8B
    - Phi 3 Medium 14B
    - Gemma 2 9B
    - Gemma 2 27B
    - Mistral 7B
    - Moondream 2 1.4B
    - Neural Chat 7B
    - Starling 7B
    - Code Llama 7B
    - Llama 2 Uncensored 7B
    - LLaVA 7B
    - Solar 10.7B

  upgradeDescription: |
    Upgrade Ollama version from v0.15.6 to v0.17.4

    ## What's Changed

    # v0.17.4 (2026-02-27)
    ## New models
    - Qwen 3.5: a family of open-source multimodal models that delivers exceptional utility and performance.
    - LFM 2: LFM2 is a family of hybrid models designed for on-device deployment. LFM2-24B-A2B is the largest model in the family.

    ## Fixes
    - Tool call indices will now be included in parallel tool calls

    # v0.17.3
    - Fixed issue where tool calls in the Qwen 3 and Qwen 3.5 model families would not be parsed correctly if emitted during thinking

    # v0.17.2
    - Fixed issue where Ollama's app on Windows would crash when a new update has been downloaded

    # v0.17.1
    - Nemotron architecture support in Ollama's engine
    - MLX engine now has improved memory usage
    - Ollama's app will now allow models that support tools to use web search capabilities
    - Improved LFM2 and LFM2.5 models in Ollama's engine
    - `ollama create` will no longer default to affine quantization for unquantized models when using the MLX engine
    - Added configuration for disabling automatic update downloading

    # v0.17.0
    - OpenClaw can now be installed and configured automatically via Ollama (`ollama launch openclaw`)
    - Improved tokenizer performance
    - Ollama's macOS and Windows apps will now default to a context length based on available VRAM

    For detailed release notes including new models and features in each version, please visit:
    https://github.com/ollama/ollama/releases
